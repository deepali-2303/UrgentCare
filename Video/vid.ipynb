{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ef78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### General imports ###\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "\n",
    "### Image processing ###\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.spatial import distance\n",
    "import imutils\n",
    "from scipy import ndimage\n",
    "\n",
    "import dlib\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils import face_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0b3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Recording audio...\n",
      "Finished recording audio.\n",
      "Top Unique Emotions Detected: []\n",
      "Mean Stress Level: 0.00%\n",
      "The emotions detected are . Approximate Stress Level: 0.00%\n"
     ]
    }
   ],
   "source": [
    "emotions_over_time = []\n",
    "stress_levels_over_time = []\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 30\n",
    "WAVE_OUTPUT_FILENAME = \"output_audio.wav\"\n",
    "# label = \"NONE\"\n",
    "s = \"None\"\n",
    "def show_webcam() :\n",
    "    global s\n",
    "    global emotions_over_time\n",
    "    global stress_levels_over_time\n",
    "#     global label\n",
    "    shape_x = 48\n",
    "    shape_y = 48\n",
    "    input_shape = (shape_x, shape_y, 1)\n",
    "    nClasses = 7\n",
    "\n",
    "    emotion_weights = {\n",
    "        \"Angry\": 0.725,\n",
    "        \"Disgust\": 0.5775,\n",
    "        \"Fear\": 0.945,\n",
    "        \"Happy\": 0.0825,\n",
    "        \"Sad\": 0.835,\n",
    "        \"Surprise\": 0.4125,\n",
    "        \"Neutral\": 0.2475  \n",
    "    }\n",
    "    \n",
    "#     index = {'Abrasions': 0, 'Bruises': 1, 'Burn_or_Stabs': 2, 'Cut_or_Laceration': 3}\n",
    "    \n",
    "    \n",
    "    thresh = 0.25\n",
    "    frame_check = 20\n",
    "\n",
    "    \n",
    "\n",
    "    def detect_face(frame):\n",
    "        \n",
    "        #Cascade classifier pre-trained model\n",
    "        cascPath = 'Models/face_landmarks.dat'\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "        \n",
    "        #BGR -> Gray conversion\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #Cascade MultiScale classifier\n",
    "        detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n",
    "                                                      minSize=(shape_x, shape_y),\n",
    "                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        coord = []\n",
    "                                                      \n",
    "        for x, y, w, h in detected_faces :\n",
    "            if w > 100 :\n",
    "                sub_img=frame[y:y+h,x:x+w]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n",
    "                coord.append([x,y,w,h])\n",
    "\n",
    "        return gray, detected_faces, coord\n",
    "\n",
    "    def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
    "        gray = faces[0]\n",
    "        detected_face = faces[1]\n",
    "        \n",
    "        new_face = []\n",
    "        \n",
    "        for det in detected_face :\n",
    "            \n",
    "            x, y, w, h = det\n",
    "            \n",
    "            horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "            vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "            \n",
    "            \n",
    "            extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "            \n",
    "            \n",
    "            new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
    "            \n",
    "            new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "            \n",
    "            new_extracted_face /= float(new_extracted_face.max())\n",
    "            \n",
    "            \n",
    "            new_face.append(new_extracted_face)\n",
    "        \n",
    "        return new_face\n",
    "\n",
    "\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    \n",
    "    (nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"nose\"]\n",
    "    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "    (jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "\n",
    "    (eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "    (ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
    "\n",
    "    model = load_model('Models/video.h5')\n",
    "#     model_injury = load_model('cnn.h5')\n",
    "    face_detect = dlib.get_frontal_face_detector()\n",
    "    predictor_landmarks  = dlib.shape_predictor(\"Models/face_landmarks.dat\")\n",
    "    \n",
    "    audio_frames = []\n",
    "    video_closed = False\n",
    "\n",
    "    def record_audio():\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                            rate=RATE, input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "\n",
    "        print(\"Recording audio...\")\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            audio_frames.append(data)\n",
    "            if video_closed:\n",
    "                break\n",
    "\n",
    "        print(\"Finished recording audio.\")\n",
    "        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "   \n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    \n",
    "    \n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    audio_thread.start()\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        \n",
    "        face_index = 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        \n",
    "#         test_image = gray.copy()\n",
    "#         test_image = cv2.resize(test_image, (64, 64))\n",
    "#         test_image = tf.keras.utils.img_to_array(test_image)\n",
    "#         test_image = np.expand_dims(test_image, axis = 0)\n",
    "#         output = model_injury.predict(test_image)\n",
    "        \n",
    "#         predicted_class_index = output.argmax()\n",
    "#         labels = list(index.keys())\n",
    "#         label = labels[predicted_class_index]\n",
    "        \n",
    "        rects = face_detect(gray, 1)\n",
    "        #gray, detected_faces, coord = detect_face(frame)\n",
    "\n",
    "        for (i, rect) in enumerate(rects):\n",
    "\n",
    "            shape = predictor_landmarks(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            \n",
    "            # Identify face coordinates\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            face = gray[y:y+h,x:x+w]\n",
    "            \n",
    "            #Zoom on extracted face\n",
    "            face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n",
    "            \n",
    "            \n",
    "            face = face.astype(np.float32)\n",
    "            \n",
    "            \n",
    "            face /= float(face.max())\n",
    "            face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "            \n",
    "           \n",
    "            prediction = model.predict(face)\n",
    "            prediction_result = np.argmax(prediction)\n",
    "            \n",
    "            top_three_emotions = np.argsort(prediction[0])[-3:]\n",
    "\n",
    "            \n",
    "            top_three_emotion_labels = [list(emotion_weights.keys())[i] for i in top_three_emotions]\n",
    "\n",
    "            \n",
    "            stress_level = sum(prediction[0][i] * emotion_weights[emotion] for i, emotion in zip(top_three_emotions, top_three_emotion_labels))\n",
    "\n",
    "            emotions_over_time.append(prediction_result)\n",
    "            stress_levels_over_time.append(stress_level)\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Stress Level: {:.2f}%\".format(stress_level * 100), (frame.shape[1] - 200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "           \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "            cv2.putText(frame, \"Face #{}\".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "     \n",
    "            for (j, k) in shape:\n",
    "                cv2.circle(frame, (j, k), 1, (0, 0, 255), -1)\n",
    "            \n",
    "           \n",
    "            cv2.putText(frame, \"----------------\",(40,100 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Emotional report : Face #\" + str(i+1),(40,120 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Angry : \" + str(round(prediction[0][0],3)),(40,140 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Disgust : \" + str(round(prediction[0][1],3)),(40,160 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Fear : \" + str(round(prediction[0][2],3)),(40,180 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Happy : \" + str(round(prediction[0][3],3)),(40,200 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Sad : \" + str(round(prediction[0][4],3)),(40,220 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Surprise : \" + str(round(prediction[0][5],3)),(40,240 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Neutral : \" + str(round(prediction[0][6],3)),(40,260 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            \n",
    "           \n",
    "            if prediction_result == 0 :\n",
    "                cv2.putText(frame, \"Angry\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 1 :\n",
    "                cv2.putText(frame, \"Disgust\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 2 :\n",
    "                cv2.putText(frame, \"Fear\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 3 :\n",
    "                cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 4 :\n",
    "                cv2.putText(frame, \"Sad\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 5 :\n",
    "                cv2.putText(frame, \"Surprise\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else :\n",
    "                cv2.putText(frame, \"Neutral\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "        \n",
    "        cv2.putText(frame,'Number of Faces : ' + str(len(rects)),(40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "  \n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    video_closed = True\n",
    "    audio_thread.join()\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(pyaudio.PyAudio().get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(audio_frames))\n",
    "    wf.close()\n",
    "s = \"The emotions detected are \"\n",
    "def main():\n",
    "    global s\n",
    "    global emotions_over_time\n",
    "    show_webcam()\n",
    "    \n",
    "    emotion_dict = {\n",
    "        0: \"Angry\",\n",
    "        1: \"Disgust\",\n",
    "        2: \"Fear\",\n",
    "        3: \"Happy\",\n",
    "        4: \"Sad\",\n",
    "        5: \"Surprise\",\n",
    "        6: \"Neutral\"  \n",
    "    }\n",
    "   \n",
    "    emotion_counts = {}\n",
    "    for emotion in emotions_over_time:\n",
    "        if emotion in emotion_counts:\n",
    "            emotion_counts[emotion] += 1\n",
    "        else:\n",
    "            emotion_counts[emotion] = 1\n",
    "    \n",
    "    \n",
    "    sorted_emotions = sorted(emotion_counts, key=emotion_counts.get, reverse=True)\n",
    "    \n",
    "    top_three_emotions = []\n",
    "    for emotion in sorted_emotions:\n",
    "        if len(top_three_emotions) < 3 and emotion not in top_three_emotions:\n",
    "            top_three_emotions.append(emotion_dict[emotion])\n",
    "            s += str(emotion_dict[emotion]) + \", \" \n",
    "            \n",
    "    \n",
    "\n",
    "    if len(top_three_emotions) < 3:\n",
    "        print(\"Top Unique Emotions Detected:\", top_three_emotions)\n",
    "    else:\n",
    "        print(\"Top Three Unique Emotions Detected:\", top_three_emotions)\n",
    "    \n",
    "    if len(stress_levels_over_time) == 0 :\n",
    "        mean_stress_level = 0\n",
    "    else:\n",
    "        mean_stress_level = sum(stress_levels_over_time) / len(stress_levels_over_time)\n",
    "    print(\"Mean Stress Level: {:.2f}%\".format(mean_stress_level * 100))\n",
    "    s += \". Approximate Stress Level: {:.2f}%\".format(mean_stress_level * 100)\n",
    "    print(s)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"Injury detected:\", label)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef864b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" I'm dying, please help me.\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai/whisper-large-v3\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "text = query(\"output_audio.wav\")\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e213f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The emotions detected are . Approximate Stress Level: 0.00%\n",
      " I'm dying, please help me. On face detection The emotions detected are . Approximate Stress Level: 0.00%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as palm\n",
    "palm.configure(api_key=\"YOUR_API_KEY\")\n",
    "print(s)\n",
    "prompt = text['text']\n",
    "prompt+= \" On face detection \" + s\n",
    "print(prompt)\n",
    "response = palm.chat(context=\"You are the emergency dispatcher, the text will give you the idea about situation you have to provide immediate response which are practical\", messages=prompt)\n",
    "print(response.last)\n",
    "\n",
    "import pyttsx3\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "   \n",
    "    engine.setProperty('rate', 150) \n",
    "    engine.setProperty('volume', 0.9) \n",
    "\n",
    "    \n",
    "    engine.say(text)\n",
    "\n",
    "    \n",
    "    engine.runAndWait()\n",
    "\n",
    "text_to_speech(response.last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
